1. Vulnerabilities

    B.insert after A
	B insertion is preempted after this.next and this.prev are set
	C.insert after A runs to completion
	B insertion: A.next = B
	... node C is no longer in the (forward pointer) list (lost node)

    B.insert after A (A->Y->Z)
	B insertion is preempted after after.next is set
	Y.remove runs to completion, (A->Z)
    	B insertion: Y.prev = B
	... A->B->Y->0, many lost nodes
	... B->Y but A<-Z, inconsistency

    B.remove (A->B->C) 
	B.remove is preempted after A.next = B.next (A->C->...), (A<-B<-C<-...)
	D is inserted after A (A->D->C->...), (A<-D<-C<-...)
	B.removal: C.prev = B.prev (A->D->C->...), (A<-D; A<-C)
	... D->C but A<-C, inconsistency

    B.remove (A->B->C) 
	B.remove is preempted after A.next = B.next (A->C->...), (A<-B<-C< ...)
	D is inserted after B (A->C->D->C), (A<-B<-D<-C)
	B removal: C.prev = B.prev (A->C->D->C), (A<-C...)
	... loop in list and next/prev inconsistencies

    B.remove (A->B->A) ... EXTREMELY unlikely as list usually large
    	B.remove is preempted after test for single element list
	A.remove completes leaving B as the only element
	B.prev.next (== B.next): B.next = B.next ... no effect
	B.next.prev (== B.prev): B.prev = B.prev ... no effect

2. non-yield failure rate

	 2 threads, few failures in 20 cycles, Pfail < 5%
	 4 threads, failures in cycle 3, Pfail ~33%
	 8 threads, failures in cycle 3, Pfail ~33%
	12 threads, failures in cycle 1, Pfail ~100%

3. yield=i failure rate

	 2 threads, failures in cycles 10-15, Pfail ~8%
	 4 threads, failures in cycle 1-3, Pfail ~50%
	 8 threads, failures in cycle 1, Pfail ~100%
	12 threads, failures in cycle 1, Pfail ~100%

   	Both the missing nodes and pointer inconsistencies were predicted in part 1.

   yield=r failure rate

	 2 threads, few failures in 20 cycles, Pfail < 5%
	 4 threads, failures in cycle 3-5, Pfail ~25%
	 8 threads, failures in cycle 3, Pfail ~33%
	12 threads, failures in cycles 1-3, Pfail ~50%
	16 threads, failures in cycle 1, Pfail ~100%

	Most of the errors with remove-yields were missing nodes.
	A loop in the list could return to the head and lose all nodes past the loop

4. make DLL_Node.insert() and remove() synchronized

	some people observed measurable (e.g. 10% decreases in the error), but
	most found that any change was less than the intrinsic variability in
	the results.  It might have made a difference, but it did not eliminate
	the problem.

	(a) A synchronized method only locks "this", and hence only prevents
	    concurrent insert/remove calls on THAT PARTICULAR DLL_Node.  It does
	    not prevent other inserts or removes involving our immediate neighbors.

	(b) If you look at Tester, you will see that each node is owned by a 
	    particular thread.  No other thread will ever invoke insert or
	    remove on THAT DLL_Node.  Thus it should be expected that making
	    those methods synchronized would have no effect.

5. using synchronized blocks

	A clever solution might be to lock the prev/next neighbors as 
	well as the node whose methods are being invoked.  The problem
	is that if A->B->C an operation on B will lock B and then C,
	while a concurrent operation on C will lock C and then A ...
	resulting in a deadlock.

	A simpler and safer solution is to lock the entire list
	whenever an insert or remove is being done.  All operations
	in the DLL_Node class involve a small number of nodes.  That
	class has no notion of or reference to a "list".

	But the Tester, which makes all calls to insert and remove
	does have a list-head object.  If we synchronize insert
	and remove calls around that object, that will ensure that
	only one thread at a time is doing an insert or remove.

	Adding this code completely removed all errors (even with
	dozens of threads and hundreds of cycles).
	
Extra credit:

	Our part 3 data suggests that the yields in remove are much
	less (e.g. half) likely to result in errors than the yields insert

	There are two relatively likely reasons for this:

	   Our part 1 analysis suggested that the first yield (after testing
	   for an empty list) was unlikely to result in list corruption.  
	   This suggests that there may be only half as many unfortunate
	   preeption opportunities ... which would be consistent with a 50%
	   reduction in probability of failure.
	
	   Looking at the code in Tester.run, we see that ALL insertions are
	   done at the list head, while removals are spread throughout
	   the list.  This makes it much more likely that concurrent inserts
	   would involve the same node (vs concurrent removes).

	Either of these could explain the difference, and the combination of
	the two should surely be adequate to do so.

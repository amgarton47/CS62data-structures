In 9/19 I (Mark Kampe) had to come up to speed on how this worked, so
I read the code and made these notes.  My personal take (for what it
is worth) is:

There are three different thigns here:
   1. a JUnit adaptor that can run our test suites and capture
      the results in a per-submission json file.
   2. a bunch of python code that finds submissions, compiles them,
      runs the test suites, collects the results and (eventually)
      generates reports suitable for returning to the students.
   3. a python GUI that lets graders view the status of assignments,
      push them through the autograder and reporting processes.

Having a GUI (3) to view the status of submissions and push them
through the process is pretty nice ... but the GUI requires a huge
amount of infrastructure and as both platform and (python) version 
portability problems. 

I adapted the (2) code to simpler and more portable scripts, that can
be run one individual or groups of the assignments.  The underlying
(1) framework is unchanged, but there are now new scripts that (can)
replace the GUI.  

It involves typing to the shell, but it is simpler (to maintain), not 
much harder to use, and much more portable.  But others may not agree, 
so I am leaving these notes for others who want to work with the 
GUI/choreographer.

===========================

THE FORM OF A SUITE:
	assignment.json
		title:
		assignment:	string, expected names?
		runner:
			type: "java_single_package_runner"
				or None if autograder is not applicable
			package: for the submission AND Autograder.java
			mainclass: for manual runs of the submission

		max_score: #
		tests:[
			{name: ... method to call and parameters
			 	   or a comment if this not autograder test
			 score: # ... maximum points for passing }
		]

	_autos ... only present if autograder is applicable
		src/package/name
		   class Autograder
			... this is the meat of a JUnit test suite
			    contains all methods named in tests[] above
				    each with @Test preamble (not sure about @Rule preambles)
				    each of which culminates in a JUnit assert call
			    but does not call those methods

		src/junitmods/... three JUnit modules used to run tests, collect results

		    JSONListener(writer, collector)
			entry points called by JUnit that perform result logging
			    testStarted(descr) ... one test started
			    testFailure(failure) ... one test failed
				tell collector to log the failure
			    testFinished(descr) ... one test finished (succ or fail)
				tell collector this test is done
			    testRunFinished(result) ... all tests completed
				tell collector tests have finished
				write Gson.toJson(collector)

		    JUnitJSONObject
			the object in which results are accumulated (by the
			above entry points) and from which they are ultimately
			written out to a per-user report:
			    int total, pass, fail counts and elapsed time
			    ArrayList passes
			    ArrayList failure(name, message, trace)

		    PomonaRunner
			contains main entry point to JUnit test a submission
			    expects one argument: name of output json file

			    instantiate FileWriter, JSONListener
			    instantiate a JUnitCore test case runner
			    register JSONlistener as test case listener
			    invoke Junit runner.run(package.Autograder.class)
			    	which will invoke @Setup and every @Test in the Autograder
			    writer.close

	_output ... starts out empty
		will contain a .auto for every submission w/ raw JUnit results
		will contain a .json for every submission w/formatted results
		will eventually contain a corresponding pdf


	assign01 has an _autos/rubric.json that differs trivially from assignment.json

	assign11 does not have the above structure, but rather test.sh & makefile

	maps github IDs (in assignment names) to students for records

SUITE OUTPUT:	under _output in the grading directory
	
	a pdf for each student w/name <githubid>.pdf
	   a prettier version of the <githubid>.json file
	   	Rubric:
		   test name
		   Score: earned/possible
		   Comments:

		   ...

		Final Score:
		   total/possible

	 	General comments:
		   ...

	    each source code module

	a summary results file for each student w/name <githubid>.json
	 tests: {
	     "testname": {
		"comment": "... if you want to add something",
		"earned": points,
		"score": possible-points,
		"_id": # ... internal sequence number
	     },
	     ...
	 },
	 "flags": ["flag_quality", "flag_general", "flag_dishonesty", "flag_regrade"]
	 "earned_score": points
	}

	# a JUnit report w/name <githubid>.autos
	{    "totalcount": #, test cases run,
	     "passed_count": #, test cases passed
	     "failed_count": #, test cases failed
	     "time": #, in some unit
	     "version": "0.0.1",
	     "failures":["list", "of", "failed", "cases"],
	     "passes":["list", "of", "passed", "cases"]
	}


	
CONFIGURATION FILE: 
	classroom.json {
	    account: { email: str, name: str, full_name: str },
	}

OVERVIEW OF THE AUTOGRADER CODE (in ag/grader):
ag/templates ... for the Flask web server

    base.html ... looks like all administrative imports

    welcome.html ... extends base
	instructions on how to pre-process ... might no longer be needed
	run unit tests (on all or some student?)
        complete human grading (lots of instructions)

    grading.html
	ui_container
	    ui_error message
	    ui segment (runner buttons)
	    ui header (submission:)
	    rubric
	    	ui segment
		    ui header ... instructor flags
		    ...
		ui celled striped table
		    table of results, w/calc and pencil icons

    rubric.html
    	student name, username, results


ag/grader

    #
    # application ... digest class/test info and start the server
    #
    #	setup
    #	    read in list of students (classroom.json or --classpath filename)
    #	    read in the suite description (assignment.json)
    #		title, assgt, info, runner, writeup
    #		dict {name: {score:, id: }
    #	    note where we are supposed to put our output
    #	for each student
    #	    see if (s)he is already done (look for _output/<githubid>.json)
    #	    add a Student record to classroom dict
    #		githubID: (name, fullname, email, done-yet)
    #
    #	start server with that classroom dict and test suite description
    #	    ... sort the student names in the classroom dict
    #	    ... create _output directory if needed
    #	    ... see if there is an _autos (JUnit) directory
    #	    ... kick off a Flask (web server, see templates and server.py)
    #		which will initiate all subsequent processing
    #	    ... kick off a JavaPackageRunner
    #
    application.py (main method)
    	classpath = cwd/classroom.json (override w/--classpath)
	assignment_path = assignment.json
	output_path = output

	specs = Specifications(assignment_path)

	data = json.load(classroom_path)
	for i in data
		complete = that students output is already there
		classroom[i] = student(i, data[i].name, data[i].email, complete)
	runserver(classroom, specs)

    #
    # Server.py runs a Flask web-server, who's actions result in
    # 	1. calls back into server.py 
    #   2. updates to our dict of student results
    #
    #   grade button (for a particular student):
    #	    assemble, build, run PomnaRunner on this submission
    #	    capture the output in <githubid>.autos
    #	    update student scores for each reported test in student scores dict
    #	    throw up a grading page to report those results
    #
    #   run button (for a particular student):
    #	    invoke the specified app's main method
    #
    #   rubric button (for a particular student)
    #	    turn the accumulated scores into
    #		_output/<githubid>.json
    #		_output/<githubid>.pdf
    #		displayed web page
    #
    server.py
	instantiate the specified JavaPackageRunner 
	app.run()

	student_page(name) ... Flask:POST->/grade/<githubid>
	   instantiate the submission
	   jp.run_autos
	   fix_permissions on the output (<githubid>.autos)
	   jp.destroycurrent_build
	   report success or failure of finding expected output
	   read in the produced <GID>.autos
	   spawn a (MacOS) open of ?the submission directory?

	   if we did not pass and we have pre-existing <GID>.autos
	   	read that back in instead
	   if we have results (old or new)
		for each test in the <GID>.autos
		    if in passed list
			add earned to our student results dict
			add 'automatically passed' as comment in our results dict
	   	    else
		    	find it in the failures list
	   		add earned 0 to our student results dict
			add comment from failure (if one) to our student results dict
	   render_template(grading.html) for this student

	run_submission(name): ... Flask:POST->/grade/run/<githubid>
	   instantiate the Submission
	   	filepath, assignment, empty list of students
	   if there is such a submission directory, and <githubid> in classroom
	   	pick up any CLI args that accompanied the request
		use the JavaPackageRunner run_manual method to simply run the submitted program
	        jp.destroy_current_build

    	save_rubric(name): ... Flask:POST->/grade/<githubid>/rubric
	   if <GID> in classroom
		create a temp directory
		generate names for the <GID>.pdf and <GID>.json files
		make (deep) copy of the list of tests
		score = 0, possible = 0
		for every test in the list of tests
		    pick up the earned and comment fields
		    add to score and possible
	   	for any flags in the request
	   	    append that flag to the record

		create the <GID>.json file
		json.dump the accumulated results dict
		      tests ... w/name, comment, earned, score, id
		      flags
		      earned score
		fix its protections
		render and display the accumulated results (rubric.html)
		create a pdf from the same html
		copy it into the output directory
		fix the permissions on the pdf

	get_source_files
	   find all files that end with specified sufix

	make_pdf(pdf_source, ...)
	    run a command to MacOS.open the pdf file

	pdfsource
	   enscript a bunch of stuff

	enscript()
	   create a pdf for a bunch of stuff

    runhelpers.py
	class Specifications:
		instantiate all info in assignment.json
	class Student(username, fullname, email, complete):
	class Classroom:
		add_student()
	class Submission(filepath, assignment_object):

	class JavaPackageRunner(pacakge, mainclass, unittests=None):
	   build_submission(submission)
	   	destroy current build
		create temp directory
		_dir_scaffold()
		re-copy file with all private methods becoming protected
		if unittests:
			wildcard_copy junitmodes
			wildcard_copy dependecies
			wildcard_copy src
		javac all the .java files in the temp dir

	   run_autos(outfile, submission=None)
	   	_check_build(submission)
		java -Dtesting -cp junitmods/PomonaRUnner

	   run_manual(submission=None, args=None)
	   	java -Dtesting -cp pkg/mainclass

	   _check_build
	   	if not built build_submission(submission)

	   destroy_current_build
	   	delete the build tree

	   class JavaPackageRunnerException:
	   	__init__ and __str__

	   dir_scaffold
		create temporary bin, src, dependencies, junitmods directories 
		(I assume so that we can copy in to them)

	convertHtmlToPdf
		create output and pisa.CreatePDF(source, dest)

    # did not fully understand, but it looks like it collects assignments
    # for grading, and reports on what it did and did not find)
    scraper.py
	namebits(str) ???
	findcollabs(classroom, name_parts) ???
	get_latest_paths(clssroom, root, manifest_name)
		complex, many tricks to find a manifest file
	copy_paths_with_rename(classroom, destination)
		for each student, copy their tree to a destination
	list_collaborators(classroom)
		perhaps update status of collaborators of known submissions?
	main source destination manifest classroom.json
		get_latest_paths
		copy_paths_with_rename
			create that script
			add the collaborators
		print out a list of the people we copied
		print out a list of the people we failed to copy
		print out a list of people who weren't processed


    setuputilities.py
	make_temp_directory: create, and after done delete a temp directory
	wildcard_copy: copy everything in a directory to somewhere else
	find_ext: walk to find java sources
	fixpermissions:	make it 770, group=cs062

    summary.py
        merge(dir, csvpath):
	    open the csv file
	    read int into a dict
	    append all of that stuff to original_csv

	    open csv file for write
	    write out a header
	    for each row
	    	load the result file
		write output

        main directory-of-processed-json-files csv-with-class-info
	    merge(dir, csvfile)
